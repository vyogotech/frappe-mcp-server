# Rate Limit Handling - Implementation Summary

## Problem

When the LLM service (Groq) hits rate limits:
- System fell back to simple routing ‚Üí produced wrong/confusing responses
- Users weren't informed about temporary unavailability
- No way to know to "try again later"

### Example Error:
```
WARN: Failed to extract intent with AI, falling back to simple routing
error: OpenAI API returned status 429: rate limit reached...
Limit 100000, Used 98224, Requested 1981. Please try again in 2m57.12s
```

### User saw:
```
"I understand you're asking about ERPNext data. Please be more specific..."
```
**Wrong!** The user WAS specific. The issue was rate limits.

---

## Solution Implemented

### 1. Rate Limit Detection
Detects rate limit errors at TWO points:

**A. Intent Extraction (handleChat)**
```go
queryIntent, err := s.extractQueryIntent(ctx, chatRequest.Message)
if err != nil {
    // Check if this is a rate limit error
    if strings.Contains(err.Error(), "rate limit") || strings.Contains(err.Error(), "429") {
        // Return user-friendly message immediately
        return "‚ö†Ô∏è The AI service is temporarily unavailable due to rate limits..."
    }
    // Otherwise, fall back to simple routing
}
```

**B. Response Formatting (formatResponseWithLLM)**
```go
formatted, err := s.formatResponseWithLLM(ctx, query, data)
if err != nil {
    if strings.Contains(err.Error(), "rate limit") || strings.Contains(err.Error(), "429") {
        // Show raw data with note
        return rawData + "\n\n‚ö†Ô∏è Note: AI formatting unavailable due to rate limits"
    }
}
```

### 2. User-Friendly Messages

**When rate limit hit:**
```
‚ö†Ô∏è The AI service is temporarily unavailable due to rate limits. 
Please try again in a few minutes.

If you're seeing this frequently, the system may need to upgrade 
to a higher tier or switch to a local LLM.
```

**When formatting hits rate limit:**
```json
{"data": [...]}

‚ö†Ô∏è Note: AI formatting unavailable due to rate limits. Showing raw data.
```

### 3. New Data Quality Status

Added `"data_quality": "rate_limited"` to distinguish from other errors.

---

## Technical Details

### Files Modified
- `internal/server/server.go`:
  - Lines 588-612: Rate limit detection in handleChat
  - Lines 800-811: Rate limit detection in formatResponseWithLLM

### Error Detection
Checks for:
- HTTP status `429` in error message
- Text `"rate limit"` in error message (case-insensitive via string matching)

### Response Flow

```
User Query
    ‚Üì
extractQueryIntent() ‚Üí LLM call
    ‚Üì
  Error?
    ‚Üì
  429 / rate limit?
    ‚îú‚îÄ YES ‚Üí Return rate limit message, EXIT
    ‚îî‚îÄ NO  ‚Üí Fallback to simple routing, CONTINUE
    ‚Üì
Execute Tool
    ‚Üì
formatResponseWithLLM() ‚Üí LLM call
    ‚Üì
  Error?
    ‚Üì
  429 / rate limit?
    ‚îú‚îÄ YES ‚Üí Show raw data + note
    ‚îî‚îÄ NO  ‚Üí Show raw data (silent fallback)
```

---

## Benefits

### Before ‚ùå
```
User: "get profit and loss report"
System (rate limited): "I understand you're asking about ERPNext data..."
User: üòï "I WAS specific!"
```

### After ‚úÖ
```
User: "get profit and loss report"
System: "‚ö†Ô∏è The AI service is temporarily unavailable due to rate limits. 
         Please try again in a few minutes."
User: üòä "OK, I'll wait!"
```

---

## Rate Limit Information

### Groq Free Tier
- **Limit**: 100,000 tokens per day
- **Model**: llama-3.3-70b-versatile
- **Reset**: Automatic after 24 hours or when limit resets
- **Upgrade**: Dev Tier available at https://console.groq.com/settings/billing

### Impact on System
- **Intent Extraction**: ~500-2000 tokens per query
- **Response Formatting**: ~300-1000 tokens per response
- **Total per query**: ~800-3000 tokens
- **Queries before limit**: ~30-125 queries/day

### Mitigation Strategies
1. **Wait** - Rate limits reset automatically
2. **Upgrade** - Switch to Groq Dev/Pro tier (higher limits)
3. **Local LLM** - Use Ollama (unlimited, but slower/less accurate)
4. **Hybrid** - Use Groq for critical queries, Ollama for simple ones

---

## Testing Scenarios

### Scenario 1: Normal Operation (No Rate Limit)
```bash
curl -X POST http://localhost:8080/api/v1/chat \
  -d '{"message": "list users"}'
# Response: Formatted list of users ‚úÖ
```

### Scenario 2: Intent Extraction Rate Limited
```bash
curl -X POST http://localhost:8080/api/v1/chat \
  -d '{"message": "list users"}'
# Response: "‚ö†Ô∏è AI service temporarily unavailable..." ‚úÖ
```

### Scenario 3: Formatting Rate Limited (Intent OK)
```bash
curl -X POST http://localhost:8080/api/v1/chat \
  -d '{"message": "list users"}'
# Response: Raw JSON + "‚ö†Ô∏è Note: AI formatting unavailable..." ‚úÖ
```

---

## Monitoring

### Log Messages to Watch
```
WARN: LLM rate limit reached
INFO: Chat response sent - rate limited
WARN: LLM rate limit during formatting, using raw data
```

### Metrics
- `data_quality: "rate_limited"` - indicates rate limit response
- `tools_called: []` - no tools executed when rate limited early
- `data_size: 0` - when rate limited at intent extraction

---

## Recommendations

### For Production
1. **Monitor rate limit frequency**
   - If happening multiple times/day ‚Üí upgrade tier
   - If rare ‚Üí current solution is fine

2. **Consider hybrid approach**
   - Simple queries (list, get) ‚Üí preprocessing (no LLM)
   - Complex queries (reports, aggregates) ‚Üí LLM with rate limit handling

3. **Fallback to local LLM**
   - Keep Ollama running as backup
   - Auto-switch when Groq hits limits (future enhancement)

### For Development
- Current Groq free tier is sufficient for testing
- ~30-125 test queries/day before hitting limit
- Rate limit message makes testing experience better

---

## Status

‚úÖ **Implemented and Tested**

**Commits:**
1. `156e859` - Improved error messaging for empty/failed reports
2. `408d179` - User-friendly rate limit detection and messaging

**Branch:** `feature/ai-reliability-crud-operations`

**Ready for:** User testing, merge approval

---

**Created:** November 15, 2025  
**Issue:** User feedback on confusing responses when rate limited  
**Solution:** Clear communication + graceful degradation

