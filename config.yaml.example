# Frappe MCP Server Configuration
server:
  host: "0.0.0.0"
  port: 8080
  timeout: "30s"
  max_connections: 100

erpnext:  # Frappe instance configuration (works with ERPNext, Frappe HR, Healthcare, etc.)
  base_url: "http://localhost:8000"
  # API key and secret are OPTIONAL when OAuth2 is enabled (auth.enabled: true and auth.require_auth: true)
  # If not provided, the server will pass through user OAuth2 tokens for user-level permissions
  # If provided, they will be used as fallback when no user token is available
  api_key: "your_api_key_here"
  api_secret: "your_api_secret_here"
  timeout: "30s"
  rate_limit:
    requests_per_second: 10
    burst: 20
  retry:
    max_attempts: 5
    initial_delay: "500ms"
    max_delay: "5s"

logging:
  level: "info"
  format: "json"

# LLM Provider Configuration - Generic and Flexible!
# Works with ANY OpenAI-compatible API: Ollama, OpenAI, Together.ai, Groq, etc.
llm:
  # Provider type: "openai-compatible" (default), "anthropic", or "azure"
  provider_type: "openai-compatible"
  
  # RECOMMENDED: Groq (ultra-fast, reliable, free tier available)
  base_url: "https://api.groq.com/openai/v1"  # Groq API endpoint
  api_key: "YOUR_GROQ_API_KEY_HERE"           # Get from https://console.groq.com
  model: "llama-3.3-70b-versatile"            # 70B model - very reliable
  timeout: "30s"                              # Request timeout (Groq is fast!)
  max_tokens: 1000                            # Max tokens in response
  temperature: 0.3                            # Lower temp for consistent output
  
  # Alternative: Local Ollama (private but less reliable)
  # base_url: "http://localhost:11434/v1"
  # api_key: ""
  # model: "llama3.1:latest"
  # timeout: "60s"
  # max_tokens: 500
  # temperature: 0.7
  
  # Azure-specific (only if provider_type is "azure")
  # azure_deployment: "gpt-4"
  # azure_api_version: "2024-02-01"

# Examples - Just change base_url, api_key, and model:
#
# Ollama (local, free, private):
#   base_url: "http://localhost:11434/v1"
#   api_key: ""
#   model: "llama3.2:1b"
#
# OpenAI:
#   base_url: "https://api.openai.com/v1"
#   api_key: "${LLM_API_KEY}"
#   model: "gpt-4o-mini"
#
# Together.ai (150+ open-source models):
#   base_url: "https://api.together.xyz/v1"
#   api_key: "${LLM_API_KEY}"
#   model: "meta-llama/Meta-Llama-3.1-70B-Instruct-Turbo"
#
# Groq (ultra-fast inference):
#   base_url: "https://api.groq.com/openai/v1"
#   api_key: "${LLM_API_KEY}"
#   model: "llama-3.1-70b-versatile"
#
# OpenRouter (200+ models):
#   base_url: "https://openrouter.ai/api/v1"
#   api_key: "${LLM_API_KEY}"
#   model: "anthropic/claude-3.5-sonnet"
#
# LocalAI (self-hosted):
#   base_url: "http://localhost:8080/v1"
#   api_key: ""
#   model: "llama-3"
#
# Anthropic Claude (different API):
#   provider_type: "anthropic"
#   base_url: "https://api.anthropic.com/v1"
#   api_key: "${LLM_API_KEY}"
#   model: "claude-3-5-sonnet-20241022"
#
# Azure OpenAI:
#   provider_type: "azure"
#   base_url: "https://your-resource.openai.azure.com"
#   api_key: "${LLM_API_KEY}"
#   model: "gpt-4"
#   azure_deployment: "gpt-4"
#   azure_api_version: "2024-02-01"

cache:
  ttl: "300s"
  max_size: 1000

performance:
  worker_pool_size: 10
  batch_size: 50
  enable_compression: true

# Authentication Configuration
auth:
  # Enable/disable authentication
  enabled: true
  
  # Require authentication for all endpoints (false = optional auth for backward compatibility)
  require_auth: true
  
  # OAuth2 Configuration
  oauth2:
    # Frappe OAuth2 endpoints
    token_info_url: "http://localhost:8000/api/method/frappe.integrations.oauth2.openid.userinfo"
    issuer_url: "http://localhost:8000"
    
    # Trusted backend clients (can provide user context via headers)
    trusted_clients:
      - "frappe-mcp-backend"
    
    # Validate tokens with remote OAuth2 provider (false = skip validation for dev)
    validate_remote: true
    
    # HTTP client timeout for token validation
    timeout: "30s"
  
  # Token cache configuration
  token_cache:
    ttl: "5m"              # How long to cache validated tokens
    cleanup_interval: "10m" # How often to clean up expired tokens

# Production Auth Example with OAuth2 Token Pass-through:
#
# When auth is enabled and required, user OAuth2 tokens are passed through to Frappe.
# This maintains user-level permissions - each request is executed with the user's own credentials.
# API key/secret can be omitted or left as fallback for system-level operations.
#
# auth:
#   enabled: true
#   require_auth: true
#   oauth2:
#     token_info_url: "https://your-frappe-instance.com/api/method/frappe.integrations.oauth2.openid.userinfo"
#     issuer_url: "https://your-frappe-instance.com"
#     trusted_clients:
#       - "frappe-mcp-backend"
#       - "mobile-app-client"
#     validate_remote: true
#     timeout: "30s"
#   token_cache:
#     ttl: "5m"
#     cleanup_interval: "10m"
#
# erpnext:  # Frappe instance config
#   base_url: "https://your-frappe-instance.com"
#   api_key: ""      # Optional - will use user OAuth2 tokens
#   api_secret: ""   # Optional - will use user OAuth2 tokens
#   timeout: "30s"
